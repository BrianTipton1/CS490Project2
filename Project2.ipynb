{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_length(filepath):\n",
    "    max_length = 0\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            _, seq = line.split(maxsplit=1)\n",
    "            seq = seq.strip()\n",
    "            if len(seq) > max_length:\n",
    "                max_length = len(seq)\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence):\n",
    "    mapping = {\n",
    "        \"A\": [1, 0, 0, 0],\n",
    "        \"T\": [0, 1, 0, 0],\n",
    "        \"G\": [0, 0, 1, 0],\n",
    "        \"C\": [0, 0, 0, 1],\n",
    "    }\n",
    "    return np.array(\n",
    "        [mapping[char.upper()] for char in sequence.strip() if char.upper() in mapping],\n",
    "        dtype=float,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(pos_dir, neg_dir, output_file, limit=None):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        pos_files = sorted(os.listdir(pos_dir))\n",
    "        neg_files = sorted(os.listdir(neg_dir))\n",
    "        random.shuffle(pos_files)\n",
    "        random.shuffle(neg_files)\n",
    "\n",
    "        half_limit = len(pos_files) if limit is None else limit // 2\n",
    "        half_limit = min(half_limit, len(pos_files), len(neg_files))\n",
    "\n",
    "        for pos_file in pos_files[:half_limit]:\n",
    "            with open(os.path.join(pos_dir, pos_file), \"r\") as pf:\n",
    "                f.writelines([\"1 \" + line for line in pf])\n",
    "\n",
    "        for neg_file in neg_files[:half_limit]:\n",
    "            with open(os.path.join(neg_dir, neg_file), \"r\") as nf:\n",
    "                f.writelines([\"0 \" + line for line in nf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(input_file, limit=None):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "\n",
    "        n = len(lines) if limit is None else limit\n",
    "\n",
    "        train_end = int(n * 0.8)\n",
    "        train_lines = lines[:train_end]\n",
    "        test_lines = lines[train_end:]\n",
    "\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.writelines(train_lines)\n",
    "    with open(\"test.txt\", \"w\") as f:\n",
    "        f.writelines(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, filepath, max_samples=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        count = 0\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                if max_samples and count >= max_samples:\n",
    "                    break\n",
    "                label, seq = line.split(maxsplit=1)\n",
    "                encoded_seq = one_hot_encode(seq.strip())\n",
    "                tensor_seq = torch.tensor(encoded_seq, dtype=torch.float).transpose(\n",
    "                    0, 1\n",
    "                )\n",
    "                self.data.append(tensor_seq)\n",
    "                self.labels.append(int(label))\n",
    "                count += 1\n",
    "\n",
    "        self.data = torch.stack(self.data)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(filepath, batch_size, max_samples=None):\n",
    "    dataset = MyDataset(filepath, max_samples)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCNN(nn.Module):\n",
    "    def __init__(self, input_length):\n",
    "        super(MiniCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(6, 16, kernel_size=3)\n",
    "\n",
    "        def conv_output_size(L, K, S=1, P=0):\n",
    "            return (L - K + 2 * P) // S + 1\n",
    "\n",
    "        L1 = conv_output_size(input_length, 5)\n",
    "        L2 = conv_output_size(L1, 2, S=2)\n",
    "        L3 = conv_output_size(L2, 3)\n",
    "        L4 = conv_output_size(L3, 2, S=2)\n",
    "\n",
    "        fc_input_features = 16 * L4\n",
    "        self.fc1 = nn.Linear(fc_input_features, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, input_length):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 96, kernel_size=11, stride=4)\n",
    "        self.local_response1 = nn.LocalResponseNorm(\n",
    "            size=5, alpha=0.0001, beta=0.75, k=2\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv1d(96, 256, kernel_size=5, padding=2)\n",
    "        self.local_response2 = nn.LocalResponseNorm(\n",
    "            size=5, alpha=0.0001, beta=0.75, k=2\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv1d(256, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(384, 384, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(384, 256, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "\n",
    "        self._to_linear = self.calculate_to_linear(input_length)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 4)\n",
    "\n",
    "    def calculate_to_linear(self, length):\n",
    "        length = (length - 11) // 4 + 1\n",
    "        length = (length - 3) // 2 + 1\n",
    "        length = (length + 2 * 2 - 5) // 1 + 1\n",
    "        length = (length - 3) // 2 + 1\n",
    "        length = (length + 2 * 1 - 3) // 1 + 1\n",
    "        length = (length + 2 * 1 - 3) // 1 + 1\n",
    "        length = (length + 2 * 1 - 3) // 1 + 1\n",
    "        length = (length - 3) // 2 + 1\n",
    "\n",
    "        return length * 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.local_response1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.local_response2(self.conv2(x))))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool5(F.relu(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optimizer, epochs):\n",
    "    model.train()\n",
    "    for batch_ids, (inputs, labels) in enumerate(train_dataloader):\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_ids + 1) % 2 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epochs,\n",
    "                    batch_ids * len(inputs),\n",
    "                    len(train_dataloader.dataset),\n",
    "                    100.0 * batch_ids / len(train_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_loss += F.nll_loss(outputs, labels, reduction=\"sum\").item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        res = \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_dataloader.dataset),\n",
    "            100.0 * correct / len(test_dataloader.dataset),\n",
    "        )\n",
    "        print(res)\n",
    "        print(\"=\" * 30)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dir = \"output/positive/cropped\"\n",
    "neg_dir = \"output/negative/cropped\"\n",
    "\n",
    "\n",
    "output_file = \"merged_data.txt\"\n",
    "\n",
    "merge_files(pos_dir, neg_dir, output_file)\n",
    "split_data(output_file)\n",
    "input_length = get_seq_length(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 SUPER\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 1024\n",
    "max_samples = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(\n",
    "    filepath=\"train.txt\", batch_size=batch_size, max_samples=max_samples\n",
    ")\n",
    "test_dataloader = create_dataloader(\n",
    "    filepath=\"test.txt\", batch_size=batch_size, max_samples=max_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1024/196512 (1%)]\tLoss: 1.329388\n",
      "Train Epoch: 1 [3072/196512 (2%)]\tLoss: 1.231537\n",
      "Train Epoch: 1 [5120/196512 (3%)]\tLoss: 1.138309\n",
      "Train Epoch: 1 [7168/196512 (4%)]\tLoss: 1.043914\n",
      "Train Epoch: 1 [9216/196512 (5%)]\tLoss: 0.952929\n",
      "Train Epoch: 1 [11264/196512 (6%)]\tLoss: 0.868485\n",
      "Train Epoch: 1 [13312/196512 (7%)]\tLoss: 0.793555\n",
      "Train Epoch: 1 [15360/196512 (8%)]\tLoss: 0.736800\n",
      "Train Epoch: 1 [17408/196512 (9%)]\tLoss: 0.710973\n",
      "Train Epoch: 1 [19456/196512 (10%)]\tLoss: 0.698382\n",
      "Train Epoch: 1 [21504/196512 (11%)]\tLoss: 0.698402\n",
      "Train Epoch: 1 [23552/196512 (12%)]\tLoss: 0.710008\n",
      "Train Epoch: 1 [25600/196512 (13%)]\tLoss: 0.697145\n",
      "Train Epoch: 1 [27648/196512 (14%)]\tLoss: 0.708362\n",
      "Train Epoch: 1 [29696/196512 (15%)]\tLoss: 0.702571\n",
      "Train Epoch: 1 [31744/196512 (16%)]\tLoss: 0.700929\n",
      "Train Epoch: 1 [33792/196512 (17%)]\tLoss: 0.710639\n",
      "Train Epoch: 1 [35840/196512 (18%)]\tLoss: 0.703513\n",
      "Train Epoch: 1 [37888/196512 (19%)]\tLoss: 0.699306\n",
      "Train Epoch: 1 [39936/196512 (20%)]\tLoss: 0.699989\n",
      "Train Epoch: 1 [41984/196512 (21%)]\tLoss: 0.707385\n",
      "Train Epoch: 1 [44032/196512 (22%)]\tLoss: 0.697883\n",
      "Train Epoch: 1 [46080/196512 (23%)]\tLoss: 0.697028\n",
      "Train Epoch: 1 [48128/196512 (24%)]\tLoss: 0.699517\n",
      "Train Epoch: 1 [50176/196512 (26%)]\tLoss: 0.698302\n",
      "Train Epoch: 1 [52224/196512 (27%)]\tLoss: 0.698878\n",
      "Train Epoch: 1 [54272/196512 (28%)]\tLoss: 0.698115\n",
      "Train Epoch: 1 [56320/196512 (29%)]\tLoss: 0.696906\n",
      "Train Epoch: 1 [58368/196512 (30%)]\tLoss: 0.699628\n",
      "Train Epoch: 1 [60416/196512 (31%)]\tLoss: 0.693819\n",
      "Train Epoch: 1 [62464/196512 (32%)]\tLoss: 0.698862\n",
      "Train Epoch: 1 [64512/196512 (33%)]\tLoss: 0.699654\n",
      "Train Epoch: 1 [66560/196512 (34%)]\tLoss: 0.693219\n",
      "Train Epoch: 1 [68608/196512 (35%)]\tLoss: 0.698477\n",
      "Train Epoch: 1 [70656/196512 (36%)]\tLoss: 0.697307\n",
      "Train Epoch: 1 [72704/196512 (37%)]\tLoss: 0.693402\n",
      "Train Epoch: 1 [74752/196512 (38%)]\tLoss: 0.695069\n",
      "Train Epoch: 1 [76800/196512 (39%)]\tLoss: 0.696377\n",
      "Train Epoch: 1 [78848/196512 (40%)]\tLoss: 0.696428\n",
      "Train Epoch: 1 [80896/196512 (41%)]\tLoss: 0.700420\n",
      "Train Epoch: 1 [82944/196512 (42%)]\tLoss: 0.699263\n",
      "Train Epoch: 1 [84992/196512 (43%)]\tLoss: 0.696816\n",
      "Train Epoch: 1 [87040/196512 (44%)]\tLoss: 0.696959\n",
      "Train Epoch: 1 [89088/196512 (45%)]\tLoss: 0.695191\n",
      "Train Epoch: 1 [91136/196512 (46%)]\tLoss: 0.691943\n",
      "Train Epoch: 1 [93184/196512 (47%)]\tLoss: 0.689255\n",
      "Train Epoch: 1 [95232/196512 (48%)]\tLoss: 0.692666\n",
      "Train Epoch: 1 [97280/196512 (49%)]\tLoss: 0.689213\n",
      "Train Epoch: 1 [99328/196512 (51%)]\tLoss: 0.681729\n",
      "Train Epoch: 1 [101376/196512 (52%)]\tLoss: 0.680681\n",
      "Train Epoch: 1 [103424/196512 (53%)]\tLoss: 0.676390\n",
      "Train Epoch: 1 [105472/196512 (54%)]\tLoss: 0.673988\n",
      "Train Epoch: 1 [107520/196512 (55%)]\tLoss: 0.671042\n",
      "Train Epoch: 1 [109568/196512 (56%)]\tLoss: 0.657422\n",
      "Train Epoch: 1 [111616/196512 (57%)]\tLoss: 0.638170\n",
      "Train Epoch: 1 [113664/196512 (58%)]\tLoss: 0.637407\n",
      "Train Epoch: 1 [115712/196512 (59%)]\tLoss: 0.662428\n",
      "Train Epoch: 1 [117760/196512 (60%)]\tLoss: 0.683490\n",
      "Train Epoch: 1 [119808/196512 (61%)]\tLoss: 0.634712\n",
      "Train Epoch: 1 [121856/196512 (62%)]\tLoss: 0.654824\n",
      "Train Epoch: 1 [123904/196512 (63%)]\tLoss: 0.641471\n",
      "Train Epoch: 1 [125952/196512 (64%)]\tLoss: 0.661279\n",
      "Train Epoch: 1 [128000/196512 (65%)]\tLoss: 0.624263\n",
      "Train Epoch: 1 [130048/196512 (66%)]\tLoss: 0.651805\n",
      "Train Epoch: 1 [132096/196512 (67%)]\tLoss: 0.629440\n",
      "Train Epoch: 1 [134144/196512 (68%)]\tLoss: 0.631356\n",
      "Train Epoch: 1 [136192/196512 (69%)]\tLoss: 0.644997\n",
      "Train Epoch: 1 [138240/196512 (70%)]\tLoss: 0.645896\n",
      "Train Epoch: 1 [140288/196512 (71%)]\tLoss: 0.639082\n",
      "Train Epoch: 1 [142336/196512 (72%)]\tLoss: 0.641096\n",
      "Train Epoch: 1 [144384/196512 (73%)]\tLoss: 0.636422\n",
      "Train Epoch: 1 [146432/196512 (74%)]\tLoss: 0.650150\n",
      "Train Epoch: 1 [148480/196512 (76%)]\tLoss: 0.644703\n",
      "Train Epoch: 1 [150528/196512 (77%)]\tLoss: 0.628275\n",
      "Train Epoch: 1 [152576/196512 (78%)]\tLoss: 0.632951\n",
      "Train Epoch: 1 [154624/196512 (79%)]\tLoss: 0.629282\n",
      "Train Epoch: 1 [156672/196512 (80%)]\tLoss: 0.640572\n",
      "Train Epoch: 1 [158720/196512 (81%)]\tLoss: 0.631707\n",
      "Train Epoch: 1 [160768/196512 (82%)]\tLoss: 0.627926\n",
      "Train Epoch: 1 [162816/196512 (83%)]\tLoss: 0.622890\n",
      "Train Epoch: 1 [164864/196512 (84%)]\tLoss: 0.644073\n",
      "Train Epoch: 1 [166912/196512 (85%)]\tLoss: 0.650987\n",
      "Train Epoch: 1 [168960/196512 (86%)]\tLoss: 0.650433\n",
      "Train Epoch: 1 [171008/196512 (87%)]\tLoss: 0.632823\n",
      "Train Epoch: 1 [173056/196512 (88%)]\tLoss: 0.629981\n",
      "Train Epoch: 1 [175104/196512 (89%)]\tLoss: 0.632152\n",
      "Train Epoch: 1 [177152/196512 (90%)]\tLoss: 0.621496\n",
      "Train Epoch: 1 [179200/196512 (91%)]\tLoss: 0.618786\n",
      "Train Epoch: 1 [181248/196512 (92%)]\tLoss: 0.634256\n",
      "Train Epoch: 1 [183296/196512 (93%)]\tLoss: 0.646347\n",
      "Train Epoch: 1 [185344/196512 (94%)]\tLoss: 0.620292\n",
      "Train Epoch: 1 [187392/196512 (95%)]\tLoss: 0.623760\n",
      "Train Epoch: 1 [189440/196512 (96%)]\tLoss: 0.640084\n",
      "Train Epoch: 1 [191488/196512 (97%)]\tLoss: 0.619872\n",
      "Train Epoch: 1 [193536/196512 (98%)]\tLoss: 0.614729\n",
      "Train Epoch: 1 [177248/196512 (99%)]\tLoss: 0.626857\n",
      "\n",
      "Test set: Average loss: -4047.1676, Accuracy: 31262/49128 (64%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 2 [1024/196512 (1%)]\tLoss: 0.638626\n",
      "Train Epoch: 2 [3072/196512 (2%)]\tLoss: 0.643672\n",
      "Train Epoch: 2 [5120/196512 (3%)]\tLoss: 0.615812\n",
      "Train Epoch: 2 [7168/196512 (4%)]\tLoss: 0.642974\n",
      "Train Epoch: 2 [9216/196512 (5%)]\tLoss: 0.638976\n",
      "Train Epoch: 2 [11264/196512 (6%)]\tLoss: 0.623571\n",
      "Train Epoch: 2 [13312/196512 (7%)]\tLoss: 0.645311\n",
      "Train Epoch: 2 [15360/196512 (8%)]\tLoss: 0.629072\n",
      "Train Epoch: 2 [17408/196512 (9%)]\tLoss: 0.636423\n",
      "Train Epoch: 2 [19456/196512 (10%)]\tLoss: 0.627350\n",
      "Train Epoch: 2 [21504/196512 (11%)]\tLoss: 0.632997\n",
      "Train Epoch: 2 [23552/196512 (12%)]\tLoss: 0.629551\n",
      "Train Epoch: 2 [25600/196512 (13%)]\tLoss: 0.639913\n",
      "Train Epoch: 2 [27648/196512 (14%)]\tLoss: 0.622763\n",
      "Train Epoch: 2 [29696/196512 (15%)]\tLoss: 0.640009\n",
      "Train Epoch: 2 [31744/196512 (16%)]\tLoss: 0.623711\n",
      "Train Epoch: 2 [33792/196512 (17%)]\tLoss: 0.628356\n",
      "Train Epoch: 2 [35840/196512 (18%)]\tLoss: 0.630092\n",
      "Train Epoch: 2 [37888/196512 (19%)]\tLoss: 0.629204\n",
      "Train Epoch: 2 [39936/196512 (20%)]\tLoss: 0.639885\n",
      "Train Epoch: 2 [41984/196512 (21%)]\tLoss: 0.647892\n",
      "Train Epoch: 2 [44032/196512 (22%)]\tLoss: 0.644469\n",
      "Train Epoch: 2 [46080/196512 (23%)]\tLoss: 0.616428\n",
      "Train Epoch: 2 [48128/196512 (24%)]\tLoss: 0.628289\n",
      "Train Epoch: 2 [50176/196512 (26%)]\tLoss: 0.621177\n",
      "Train Epoch: 2 [52224/196512 (27%)]\tLoss: 0.627917\n",
      "Train Epoch: 2 [54272/196512 (28%)]\tLoss: 0.637902\n",
      "Train Epoch: 2 [56320/196512 (29%)]\tLoss: 0.608963\n",
      "Train Epoch: 2 [58368/196512 (30%)]\tLoss: 0.624836\n",
      "Train Epoch: 2 [60416/196512 (31%)]\tLoss: 0.642127\n",
      "Train Epoch: 2 [62464/196512 (32%)]\tLoss: 0.617888\n",
      "Train Epoch: 2 [64512/196512 (33%)]\tLoss: 0.633963\n",
      "Train Epoch: 2 [66560/196512 (34%)]\tLoss: 0.612154\n",
      "Train Epoch: 2 [68608/196512 (35%)]\tLoss: 0.628526\n",
      "Train Epoch: 2 [70656/196512 (36%)]\tLoss: 0.640231\n",
      "Train Epoch: 2 [72704/196512 (37%)]\tLoss: 0.630304\n",
      "Train Epoch: 2 [74752/196512 (38%)]\tLoss: 0.619999\n",
      "Train Epoch: 2 [76800/196512 (39%)]\tLoss: 0.616942\n",
      "Train Epoch: 2 [78848/196512 (40%)]\tLoss: 0.644584\n",
      "Train Epoch: 2 [80896/196512 (41%)]\tLoss: 0.614202\n",
      "Train Epoch: 2 [82944/196512 (42%)]\tLoss: 0.631209\n",
      "Train Epoch: 2 [84992/196512 (43%)]\tLoss: 0.637404\n",
      "Train Epoch: 2 [87040/196512 (44%)]\tLoss: 0.633908\n",
      "Train Epoch: 2 [89088/196512 (45%)]\tLoss: 0.610504\n",
      "Train Epoch: 2 [91136/196512 (46%)]\tLoss: 0.634074\n",
      "Train Epoch: 2 [93184/196512 (47%)]\tLoss: 0.618046\n",
      "Train Epoch: 2 [95232/196512 (48%)]\tLoss: 0.636851\n",
      "Train Epoch: 2 [97280/196512 (49%)]\tLoss: 0.615020\n",
      "Train Epoch: 2 [99328/196512 (51%)]\tLoss: 0.613396\n",
      "Train Epoch: 2 [101376/196512 (52%)]\tLoss: 0.603874\n",
      "Train Epoch: 2 [103424/196512 (53%)]\tLoss: 0.620368\n",
      "Train Epoch: 2 [105472/196512 (54%)]\tLoss: 0.613374\n",
      "Train Epoch: 2 [107520/196512 (55%)]\tLoss: 0.630343\n",
      "Train Epoch: 2 [109568/196512 (56%)]\tLoss: 0.624092\n",
      "Train Epoch: 2 [111616/196512 (57%)]\tLoss: 0.622860\n",
      "Train Epoch: 2 [113664/196512 (58%)]\tLoss: 0.630010\n",
      "Train Epoch: 2 [115712/196512 (59%)]\tLoss: 0.630027\n",
      "Train Epoch: 2 [117760/196512 (60%)]\tLoss: 0.618604\n",
      "Train Epoch: 2 [119808/196512 (61%)]\tLoss: 0.634150\n",
      "Train Epoch: 2 [121856/196512 (62%)]\tLoss: 0.630495\n",
      "Train Epoch: 2 [123904/196512 (63%)]\tLoss: 0.604234\n",
      "Train Epoch: 2 [125952/196512 (64%)]\tLoss: 0.645125\n",
      "Train Epoch: 2 [128000/196512 (65%)]\tLoss: 0.588786\n",
      "Train Epoch: 2 [130048/196512 (66%)]\tLoss: 0.614165\n",
      "Train Epoch: 2 [132096/196512 (67%)]\tLoss: 0.594945\n",
      "Train Epoch: 2 [134144/196512 (68%)]\tLoss: 0.620582\n",
      "Train Epoch: 2 [136192/196512 (69%)]\tLoss: 0.625266\n",
      "Train Epoch: 2 [138240/196512 (70%)]\tLoss: 0.623465\n",
      "Train Epoch: 2 [140288/196512 (71%)]\tLoss: 0.603022\n",
      "Train Epoch: 2 [142336/196512 (72%)]\tLoss: 0.619388\n",
      "Train Epoch: 2 [144384/196512 (73%)]\tLoss: 0.612717\n",
      "Train Epoch: 2 [146432/196512 (74%)]\tLoss: 0.622394\n",
      "Train Epoch: 2 [148480/196512 (76%)]\tLoss: 0.608718\n",
      "Train Epoch: 2 [150528/196512 (77%)]\tLoss: 0.611524\n",
      "Train Epoch: 2 [152576/196512 (78%)]\tLoss: 0.636651\n",
      "Train Epoch: 2 [154624/196512 (79%)]\tLoss: 0.615697\n",
      "Train Epoch: 2 [156672/196512 (80%)]\tLoss: 0.644711\n",
      "Train Epoch: 2 [158720/196512 (81%)]\tLoss: 0.615274\n",
      "Train Epoch: 2 [160768/196512 (82%)]\tLoss: 0.631031\n",
      "Train Epoch: 2 [162816/196512 (83%)]\tLoss: 0.614465\n",
      "Train Epoch: 2 [164864/196512 (84%)]\tLoss: 0.614563\n",
      "Train Epoch: 2 [166912/196512 (85%)]\tLoss: 0.602612\n",
      "Train Epoch: 2 [168960/196512 (86%)]\tLoss: 0.605089\n",
      "Train Epoch: 2 [171008/196512 (87%)]\tLoss: 0.624261\n",
      "Train Epoch: 2 [173056/196512 (88%)]\tLoss: 0.621989\n",
      "Train Epoch: 2 [175104/196512 (89%)]\tLoss: 0.633575\n",
      "Train Epoch: 2 [177152/196512 (90%)]\tLoss: 0.612443\n",
      "Train Epoch: 2 [179200/196512 (91%)]\tLoss: 0.663770\n",
      "Train Epoch: 2 [181248/196512 (92%)]\tLoss: 0.622950\n",
      "Train Epoch: 2 [183296/196512 (93%)]\tLoss: 0.629581\n",
      "Train Epoch: 2 [185344/196512 (94%)]\tLoss: 0.630643\n",
      "Train Epoch: 2 [187392/196512 (95%)]\tLoss: 0.627539\n",
      "Train Epoch: 2 [189440/196512 (96%)]\tLoss: 0.630777\n",
      "Train Epoch: 2 [191488/196512 (97%)]\tLoss: 0.618306\n",
      "Train Epoch: 2 [193536/196512 (98%)]\tLoss: 0.620997\n",
      "Train Epoch: 2 [177248/196512 (99%)]\tLoss: 0.636126\n",
      "\n",
      "Test set: Average loss: -3433.3346, Accuracy: 31818/49128 (65%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 3 [1024/196512 (1%)]\tLoss: 0.623015\n",
      "Train Epoch: 3 [3072/196512 (2%)]\tLoss: 0.604464\n",
      "Train Epoch: 3 [5120/196512 (3%)]\tLoss: 0.624573\n",
      "Train Epoch: 3 [7168/196512 (4%)]\tLoss: 0.608124\n",
      "Train Epoch: 3 [9216/196512 (5%)]\tLoss: 0.611727\n",
      "Train Epoch: 3 [11264/196512 (6%)]\tLoss: 0.593260\n",
      "Train Epoch: 3 [13312/196512 (7%)]\tLoss: 0.633788\n",
      "Train Epoch: 3 [15360/196512 (8%)]\tLoss: 0.612679\n",
      "Train Epoch: 3 [17408/196512 (9%)]\tLoss: 0.608836\n",
      "Train Epoch: 3 [19456/196512 (10%)]\tLoss: 0.622311\n",
      "Train Epoch: 3 [21504/196512 (11%)]\tLoss: 0.609679\n",
      "Train Epoch: 3 [23552/196512 (12%)]\tLoss: 0.620009\n",
      "Train Epoch: 3 [25600/196512 (13%)]\tLoss: 0.618966\n",
      "Train Epoch: 3 [27648/196512 (14%)]\tLoss: 0.621314\n",
      "Train Epoch: 3 [29696/196512 (15%)]\tLoss: 0.601785\n",
      "Train Epoch: 3 [31744/196512 (16%)]\tLoss: 0.626037\n",
      "Train Epoch: 3 [33792/196512 (17%)]\tLoss: 0.612202\n",
      "Train Epoch: 3 [35840/196512 (18%)]\tLoss: 0.602954\n",
      "Train Epoch: 3 [37888/196512 (19%)]\tLoss: 0.621563\n",
      "Train Epoch: 3 [39936/196512 (20%)]\tLoss: 0.622178\n",
      "Train Epoch: 3 [41984/196512 (21%)]\tLoss: 0.607112\n",
      "Train Epoch: 3 [44032/196512 (22%)]\tLoss: 0.600443\n",
      "Train Epoch: 3 [46080/196512 (23%)]\tLoss: 0.631244\n",
      "Train Epoch: 3 [48128/196512 (24%)]\tLoss: 0.629906\n",
      "Train Epoch: 3 [50176/196512 (26%)]\tLoss: 0.600510\n",
      "Train Epoch: 3 [52224/196512 (27%)]\tLoss: 0.611427\n",
      "Train Epoch: 3 [54272/196512 (28%)]\tLoss: 0.603271\n",
      "Train Epoch: 3 [56320/196512 (29%)]\tLoss: 0.592955\n",
      "Train Epoch: 3 [58368/196512 (30%)]\tLoss: 0.619835\n",
      "Train Epoch: 3 [60416/196512 (31%)]\tLoss: 0.610415\n",
      "Train Epoch: 3 [62464/196512 (32%)]\tLoss: 0.625436\n",
      "Train Epoch: 3 [64512/196512 (33%)]\tLoss: 0.596396\n",
      "Train Epoch: 3 [66560/196512 (34%)]\tLoss: 0.606556\n",
      "Train Epoch: 3 [68608/196512 (35%)]\tLoss: 0.611925\n",
      "Train Epoch: 3 [70656/196512 (36%)]\tLoss: 0.626814\n",
      "Train Epoch: 3 [72704/196512 (37%)]\tLoss: 0.626171\n",
      "Train Epoch: 3 [74752/196512 (38%)]\tLoss: 0.615645\n",
      "Train Epoch: 3 [76800/196512 (39%)]\tLoss: 0.602466\n",
      "Train Epoch: 3 [78848/196512 (40%)]\tLoss: 0.629882\n",
      "Train Epoch: 3 [80896/196512 (41%)]\tLoss: 0.641756\n",
      "Train Epoch: 3 [82944/196512 (42%)]\tLoss: 0.614224\n",
      "Train Epoch: 3 [84992/196512 (43%)]\tLoss: 0.616220\n",
      "Train Epoch: 3 [87040/196512 (44%)]\tLoss: 0.619794\n",
      "Train Epoch: 3 [89088/196512 (45%)]\tLoss: 0.625196\n",
      "Train Epoch: 3 [91136/196512 (46%)]\tLoss: 0.605263\n",
      "Train Epoch: 3 [93184/196512 (47%)]\tLoss: 0.604229\n",
      "Train Epoch: 3 [95232/196512 (48%)]\tLoss: 0.615194\n",
      "Train Epoch: 3 [97280/196512 (49%)]\tLoss: 0.617373\n",
      "Train Epoch: 3 [99328/196512 (51%)]\tLoss: 0.570005\n",
      "Train Epoch: 3 [101376/196512 (52%)]\tLoss: 0.618497\n",
      "Train Epoch: 3 [103424/196512 (53%)]\tLoss: 0.634960\n",
      "Train Epoch: 3 [105472/196512 (54%)]\tLoss: 0.601308\n",
      "Train Epoch: 3 [107520/196512 (55%)]\tLoss: 0.586867\n",
      "Train Epoch: 3 [109568/196512 (56%)]\tLoss: 0.618752\n",
      "Train Epoch: 3 [111616/196512 (57%)]\tLoss: 0.598555\n",
      "Train Epoch: 3 [113664/196512 (58%)]\tLoss: 0.625504\n",
      "Train Epoch: 3 [115712/196512 (59%)]\tLoss: 0.604305\n",
      "Train Epoch: 3 [117760/196512 (60%)]\tLoss: 0.642052\n",
      "Train Epoch: 3 [119808/196512 (61%)]\tLoss: 0.614534\n",
      "Train Epoch: 3 [121856/196512 (62%)]\tLoss: 0.597808\n",
      "Train Epoch: 3 [123904/196512 (63%)]\tLoss: 0.615586\n",
      "Train Epoch: 3 [125952/196512 (64%)]\tLoss: 0.608135\n",
      "Train Epoch: 3 [128000/196512 (65%)]\tLoss: 0.605317\n",
      "Train Epoch: 3 [130048/196512 (66%)]\tLoss: 0.613877\n",
      "Train Epoch: 3 [132096/196512 (67%)]\tLoss: 0.586029\n",
      "Train Epoch: 3 [134144/196512 (68%)]\tLoss: 0.613150\n",
      "Train Epoch: 3 [136192/196512 (69%)]\tLoss: 0.609485\n",
      "Train Epoch: 3 [138240/196512 (70%)]\tLoss: 0.620384\n",
      "Train Epoch: 3 [140288/196512 (71%)]\tLoss: 0.617386\n",
      "Train Epoch: 3 [142336/196512 (72%)]\tLoss: 0.598220\n",
      "Train Epoch: 3 [144384/196512 (73%)]\tLoss: 0.608290\n",
      "Train Epoch: 3 [146432/196512 (74%)]\tLoss: 0.609062\n",
      "Train Epoch: 3 [148480/196512 (76%)]\tLoss: 0.608969\n",
      "Train Epoch: 3 [150528/196512 (77%)]\tLoss: 0.608506\n",
      "Train Epoch: 3 [152576/196512 (78%)]\tLoss: 0.600897\n",
      "Train Epoch: 3 [154624/196512 (79%)]\tLoss: 0.585001\n",
      "Train Epoch: 3 [156672/196512 (80%)]\tLoss: 0.621759\n",
      "Train Epoch: 3 [158720/196512 (81%)]\tLoss: 0.613135\n",
      "Train Epoch: 3 [160768/196512 (82%)]\tLoss: 0.616247\n",
      "Train Epoch: 3 [162816/196512 (83%)]\tLoss: 0.615768\n",
      "Train Epoch: 3 [164864/196512 (84%)]\tLoss: 0.620310\n",
      "Train Epoch: 3 [166912/196512 (85%)]\tLoss: 0.618198\n",
      "Train Epoch: 3 [168960/196512 (86%)]\tLoss: 0.605928\n",
      "Train Epoch: 3 [171008/196512 (87%)]\tLoss: 0.625389\n",
      "Train Epoch: 3 [173056/196512 (88%)]\tLoss: 0.602546\n",
      "Train Epoch: 3 [175104/196512 (89%)]\tLoss: 0.615983\n",
      "Train Epoch: 3 [177152/196512 (90%)]\tLoss: 0.586020\n",
      "Train Epoch: 3 [179200/196512 (91%)]\tLoss: 0.592108\n",
      "Train Epoch: 3 [181248/196512 (92%)]\tLoss: 0.629892\n",
      "Train Epoch: 3 [183296/196512 (93%)]\tLoss: 0.599522\n",
      "Train Epoch: 3 [185344/196512 (94%)]\tLoss: 0.607989\n",
      "Train Epoch: 3 [187392/196512 (95%)]\tLoss: 0.597108\n",
      "Train Epoch: 3 [189440/196512 (96%)]\tLoss: 0.589181\n",
      "Train Epoch: 3 [191488/196512 (97%)]\tLoss: 0.604986\n",
      "Train Epoch: 3 [193536/196512 (98%)]\tLoss: 0.618396\n",
      "Train Epoch: 3 [177248/196512 (99%)]\tLoss: 0.631005\n",
      "\n",
      "Test set: Average loss: -3540.0957, Accuracy: 32055/49128 (65%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 4 [1024/196512 (1%)]\tLoss: 0.614463\n",
      "Train Epoch: 4 [3072/196512 (2%)]\tLoss: 0.590090\n",
      "Train Epoch: 4 [5120/196512 (3%)]\tLoss: 0.616653\n",
      "Train Epoch: 4 [7168/196512 (4%)]\tLoss: 0.600725\n",
      "Train Epoch: 4 [9216/196512 (5%)]\tLoss: 0.624448\n",
      "Train Epoch: 4 [11264/196512 (6%)]\tLoss: 0.622599\n",
      "Train Epoch: 4 [13312/196512 (7%)]\tLoss: 0.621251\n",
      "Train Epoch: 4 [15360/196512 (8%)]\tLoss: 0.587222\n",
      "Train Epoch: 4 [17408/196512 (9%)]\tLoss: 0.603402\n",
      "Train Epoch: 4 [19456/196512 (10%)]\tLoss: 0.604603\n",
      "Train Epoch: 4 [21504/196512 (11%)]\tLoss: 0.593148\n",
      "Train Epoch: 4 [23552/196512 (12%)]\tLoss: 0.607684\n",
      "Train Epoch: 4 [25600/196512 (13%)]\tLoss: 0.610922\n",
      "Train Epoch: 4 [27648/196512 (14%)]\tLoss: 0.617724\n",
      "Train Epoch: 4 [29696/196512 (15%)]\tLoss: 0.607457\n",
      "Train Epoch: 4 [31744/196512 (16%)]\tLoss: 0.622966\n",
      "Train Epoch: 4 [33792/196512 (17%)]\tLoss: 0.605923\n",
      "Train Epoch: 4 [35840/196512 (18%)]\tLoss: 0.634263\n",
      "Train Epoch: 4 [37888/196512 (19%)]\tLoss: 0.599230\n",
      "Train Epoch: 4 [39936/196512 (20%)]\tLoss: 0.622573\n",
      "Train Epoch: 4 [41984/196512 (21%)]\tLoss: 0.614115\n",
      "Train Epoch: 4 [44032/196512 (22%)]\tLoss: 0.625276\n",
      "Train Epoch: 4 [46080/196512 (23%)]\tLoss: 0.588067\n",
      "Train Epoch: 4 [48128/196512 (24%)]\tLoss: 0.620010\n",
      "Train Epoch: 4 [50176/196512 (26%)]\tLoss: 0.597300\n",
      "Train Epoch: 4 [52224/196512 (27%)]\tLoss: 0.606115\n",
      "Train Epoch: 4 [54272/196512 (28%)]\tLoss: 0.621391\n",
      "Train Epoch: 4 [56320/196512 (29%)]\tLoss: 0.619223\n",
      "Train Epoch: 4 [58368/196512 (30%)]\tLoss: 0.612258\n",
      "Train Epoch: 4 [60416/196512 (31%)]\tLoss: 0.591255\n",
      "Train Epoch: 4 [62464/196512 (32%)]\tLoss: 0.624477\n",
      "Train Epoch: 4 [64512/196512 (33%)]\tLoss: 0.600715\n",
      "Train Epoch: 4 [66560/196512 (34%)]\tLoss: 0.610778\n",
      "Train Epoch: 4 [68608/196512 (35%)]\tLoss: 0.607079\n",
      "Train Epoch: 4 [70656/196512 (36%)]\tLoss: 0.587308\n",
      "Train Epoch: 4 [72704/196512 (37%)]\tLoss: 0.600100\n",
      "Train Epoch: 4 [74752/196512 (38%)]\tLoss: 0.600115\n",
      "Train Epoch: 4 [76800/196512 (39%)]\tLoss: 0.605324\n",
      "Train Epoch: 4 [78848/196512 (40%)]\tLoss: 0.588451\n",
      "Train Epoch: 4 [80896/196512 (41%)]\tLoss: 0.605416\n",
      "Train Epoch: 4 [82944/196512 (42%)]\tLoss: 0.597347\n",
      "Train Epoch: 4 [84992/196512 (43%)]\tLoss: 0.601780\n",
      "Train Epoch: 4 [87040/196512 (44%)]\tLoss: 0.587167\n",
      "Train Epoch: 4 [89088/196512 (45%)]\tLoss: 0.616138\n",
      "Train Epoch: 4 [91136/196512 (46%)]\tLoss: 0.613975\n",
      "Train Epoch: 4 [93184/196512 (47%)]\tLoss: 0.614555\n",
      "Train Epoch: 4 [95232/196512 (48%)]\tLoss: 0.604488\n",
      "Train Epoch: 4 [97280/196512 (49%)]\tLoss: 0.593154\n",
      "Train Epoch: 4 [99328/196512 (51%)]\tLoss: 0.610694\n",
      "Train Epoch: 4 [101376/196512 (52%)]\tLoss: 0.650472\n",
      "Train Epoch: 4 [103424/196512 (53%)]\tLoss: 0.587807\n",
      "Train Epoch: 4 [105472/196512 (54%)]\tLoss: 0.598195\n",
      "Train Epoch: 4 [107520/196512 (55%)]\tLoss: 0.582560\n",
      "Train Epoch: 4 [109568/196512 (56%)]\tLoss: 0.589988\n",
      "Train Epoch: 4 [111616/196512 (57%)]\tLoss: 0.598682\n",
      "Train Epoch: 4 [113664/196512 (58%)]\tLoss: 0.600866\n",
      "Train Epoch: 4 [115712/196512 (59%)]\tLoss: 0.597766\n",
      "Train Epoch: 4 [117760/196512 (60%)]\tLoss: 0.596541\n",
      "Train Epoch: 4 [119808/196512 (61%)]\tLoss: 0.592926\n",
      "Train Epoch: 4 [121856/196512 (62%)]\tLoss: 0.607118\n",
      "Train Epoch: 4 [123904/196512 (63%)]\tLoss: 0.609595\n",
      "Train Epoch: 4 [125952/196512 (64%)]\tLoss: 0.602084\n",
      "Train Epoch: 4 [128000/196512 (65%)]\tLoss: 0.599339\n",
      "Train Epoch: 4 [130048/196512 (66%)]\tLoss: 0.586640\n",
      "Train Epoch: 4 [132096/196512 (67%)]\tLoss: 0.582108\n",
      "Train Epoch: 4 [134144/196512 (68%)]\tLoss: 0.607002\n",
      "Train Epoch: 4 [136192/196512 (69%)]\tLoss: 0.594708\n",
      "Train Epoch: 4 [138240/196512 (70%)]\tLoss: 0.597931\n",
      "Train Epoch: 4 [140288/196512 (71%)]\tLoss: 0.609135\n",
      "Train Epoch: 4 [142336/196512 (72%)]\tLoss: 0.585958\n",
      "Train Epoch: 4 [144384/196512 (73%)]\tLoss: 0.618937\n",
      "Train Epoch: 4 [146432/196512 (74%)]\tLoss: 0.618778\n",
      "Train Epoch: 4 [148480/196512 (76%)]\tLoss: 0.585010\n",
      "Train Epoch: 4 [150528/196512 (77%)]\tLoss: 0.603164\n",
      "Train Epoch: 4 [152576/196512 (78%)]\tLoss: 0.595146\n",
      "Train Epoch: 4 [154624/196512 (79%)]\tLoss: 0.582745\n",
      "Train Epoch: 4 [156672/196512 (80%)]\tLoss: 0.604083\n",
      "Train Epoch: 4 [158720/196512 (81%)]\tLoss: 0.610897\n",
      "Train Epoch: 4 [160768/196512 (82%)]\tLoss: 0.596506\n",
      "Train Epoch: 4 [162816/196512 (83%)]\tLoss: 0.588095\n",
      "Train Epoch: 4 [164864/196512 (84%)]\tLoss: 0.602994\n",
      "Train Epoch: 4 [166912/196512 (85%)]\tLoss: 0.595740\n",
      "Train Epoch: 4 [168960/196512 (86%)]\tLoss: 0.613133\n",
      "Train Epoch: 4 [171008/196512 (87%)]\tLoss: 0.597303\n",
      "Train Epoch: 4 [173056/196512 (88%)]\tLoss: 0.594086\n",
      "Train Epoch: 4 [175104/196512 (89%)]\tLoss: 0.574669\n",
      "Train Epoch: 4 [177152/196512 (90%)]\tLoss: 0.600690\n",
      "Train Epoch: 4 [179200/196512 (91%)]\tLoss: 0.586129\n",
      "Train Epoch: 4 [181248/196512 (92%)]\tLoss: 0.570172\n",
      "Train Epoch: 4 [183296/196512 (93%)]\tLoss: 0.595873\n",
      "Train Epoch: 4 [185344/196512 (94%)]\tLoss: 0.585882\n",
      "Train Epoch: 4 [187392/196512 (95%)]\tLoss: 0.615818\n",
      "Train Epoch: 4 [189440/196512 (96%)]\tLoss: 0.588229\n",
      "Train Epoch: 4 [191488/196512 (97%)]\tLoss: 0.618487\n",
      "Train Epoch: 4 [193536/196512 (98%)]\tLoss: 0.606506\n",
      "Train Epoch: 4 [177248/196512 (99%)]\tLoss: 0.628420\n",
      "\n",
      "Test set: Average loss: -3595.2825, Accuracy: 32458/49128 (66%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 5 [1024/196512 (1%)]\tLoss: 0.602186\n",
      "Train Epoch: 5 [3072/196512 (2%)]\tLoss: 0.583261\n",
      "Train Epoch: 5 [5120/196512 (3%)]\tLoss: 0.618195\n",
      "Train Epoch: 5 [7168/196512 (4%)]\tLoss: 0.603493\n",
      "Train Epoch: 5 [9216/196512 (5%)]\tLoss: 0.607411\n",
      "Train Epoch: 5 [11264/196512 (6%)]\tLoss: 0.596084\n",
      "Train Epoch: 5 [13312/196512 (7%)]\tLoss: 0.594532\n",
      "Train Epoch: 5 [15360/196512 (8%)]\tLoss: 0.582617\n",
      "Train Epoch: 5 [17408/196512 (9%)]\tLoss: 0.601781\n",
      "Train Epoch: 5 [19456/196512 (10%)]\tLoss: 0.618940\n",
      "Train Epoch: 5 [21504/196512 (11%)]\tLoss: 0.596788\n",
      "Train Epoch: 5 [23552/196512 (12%)]\tLoss: 0.579352\n",
      "Train Epoch: 5 [25600/196512 (13%)]\tLoss: 0.596222\n",
      "Train Epoch: 5 [27648/196512 (14%)]\tLoss: 0.601277\n",
      "Train Epoch: 5 [29696/196512 (15%)]\tLoss: 0.605429\n",
      "Train Epoch: 5 [31744/196512 (16%)]\tLoss: 0.602364\n",
      "Train Epoch: 5 [33792/196512 (17%)]\tLoss: 0.612167\n",
      "Train Epoch: 5 [35840/196512 (18%)]\tLoss: 0.591445\n",
      "Train Epoch: 5 [37888/196512 (19%)]\tLoss: 0.586672\n",
      "Train Epoch: 5 [39936/196512 (20%)]\tLoss: 0.578266\n",
      "Train Epoch: 5 [41984/196512 (21%)]\tLoss: 0.615195\n",
      "Train Epoch: 5 [44032/196512 (22%)]\tLoss: 0.609572\n",
      "Train Epoch: 5 [46080/196512 (23%)]\tLoss: 0.608686\n",
      "Train Epoch: 5 [48128/196512 (24%)]\tLoss: 0.589878\n",
      "Train Epoch: 5 [50176/196512 (26%)]\tLoss: 0.587776\n",
      "Train Epoch: 5 [52224/196512 (27%)]\tLoss: 0.604113\n",
      "Train Epoch: 5 [54272/196512 (28%)]\tLoss: 0.616479\n",
      "Train Epoch: 5 [56320/196512 (29%)]\tLoss: 0.612469\n",
      "Train Epoch: 5 [58368/196512 (30%)]\tLoss: 0.589241\n",
      "Train Epoch: 5 [60416/196512 (31%)]\tLoss: 0.593997\n",
      "Train Epoch: 5 [62464/196512 (32%)]\tLoss: 0.582625\n",
      "Train Epoch: 5 [64512/196512 (33%)]\tLoss: 0.576488\n",
      "Train Epoch: 5 [66560/196512 (34%)]\tLoss: 0.593201\n",
      "Train Epoch: 5 [68608/196512 (35%)]\tLoss: 0.584303\n",
      "Train Epoch: 5 [70656/196512 (36%)]\tLoss: 0.578591\n",
      "Train Epoch: 5 [72704/196512 (37%)]\tLoss: 0.599749\n",
      "Train Epoch: 5 [74752/196512 (38%)]\tLoss: 0.589871\n",
      "Train Epoch: 5 [76800/196512 (39%)]\tLoss: 0.592709\n",
      "Train Epoch: 5 [78848/196512 (40%)]\tLoss: 0.590420\n",
      "Train Epoch: 5 [80896/196512 (41%)]\tLoss: 0.581860\n",
      "Train Epoch: 5 [82944/196512 (42%)]\tLoss: 0.608539\n",
      "Train Epoch: 5 [84992/196512 (43%)]\tLoss: 0.619873\n",
      "Train Epoch: 5 [87040/196512 (44%)]\tLoss: 0.589853\n",
      "Train Epoch: 5 [89088/196512 (45%)]\tLoss: 0.613714\n",
      "Train Epoch: 5 [91136/196512 (46%)]\tLoss: 0.580772\n",
      "Train Epoch: 5 [93184/196512 (47%)]\tLoss: 0.609228\n",
      "Train Epoch: 5 [95232/196512 (48%)]\tLoss: 0.596467\n",
      "Train Epoch: 5 [97280/196512 (49%)]\tLoss: 0.605895\n",
      "Train Epoch: 5 [99328/196512 (51%)]\tLoss: 0.607433\n",
      "Train Epoch: 5 [101376/196512 (52%)]\tLoss: 0.589083\n",
      "Train Epoch: 5 [103424/196512 (53%)]\tLoss: 0.601734\n",
      "Train Epoch: 5 [105472/196512 (54%)]\tLoss: 0.604891\n",
      "Train Epoch: 5 [107520/196512 (55%)]\tLoss: 0.577078\n",
      "Train Epoch: 5 [109568/196512 (56%)]\tLoss: 0.585226\n",
      "Train Epoch: 5 [111616/196512 (57%)]\tLoss: 0.611302\n",
      "Train Epoch: 5 [113664/196512 (58%)]\tLoss: 0.612401\n",
      "Train Epoch: 5 [115712/196512 (59%)]\tLoss: 0.603198\n",
      "Train Epoch: 5 [117760/196512 (60%)]\tLoss: 0.595595\n",
      "Train Epoch: 5 [119808/196512 (61%)]\tLoss: 0.594251\n",
      "Train Epoch: 5 [121856/196512 (62%)]\tLoss: 0.580037\n",
      "Train Epoch: 5 [123904/196512 (63%)]\tLoss: 0.595822\n",
      "Train Epoch: 5 [125952/196512 (64%)]\tLoss: 0.594418\n",
      "Train Epoch: 5 [128000/196512 (65%)]\tLoss: 0.579431\n",
      "Train Epoch: 5 [130048/196512 (66%)]\tLoss: 0.597304\n",
      "Train Epoch: 5 [132096/196512 (67%)]\tLoss: 0.598793\n",
      "Train Epoch: 5 [134144/196512 (68%)]\tLoss: 0.586148\n",
      "Train Epoch: 5 [136192/196512 (69%)]\tLoss: 0.589582\n",
      "Train Epoch: 5 [138240/196512 (70%)]\tLoss: 0.572593\n",
      "Train Epoch: 5 [140288/196512 (71%)]\tLoss: 0.562462\n",
      "Train Epoch: 5 [142336/196512 (72%)]\tLoss: 0.607474\n",
      "Train Epoch: 5 [144384/196512 (73%)]\tLoss: 0.600511\n",
      "Train Epoch: 5 [146432/196512 (74%)]\tLoss: 0.586571\n",
      "Train Epoch: 5 [148480/196512 (76%)]\tLoss: 0.585618\n",
      "Train Epoch: 5 [150528/196512 (77%)]\tLoss: 0.595526\n",
      "Train Epoch: 5 [152576/196512 (78%)]\tLoss: 0.606777\n",
      "Train Epoch: 5 [154624/196512 (79%)]\tLoss: 0.583399\n",
      "Train Epoch: 5 [156672/196512 (80%)]\tLoss: 0.567272\n",
      "Train Epoch: 5 [158720/196512 (81%)]\tLoss: 0.577371\n",
      "Train Epoch: 5 [160768/196512 (82%)]\tLoss: 0.595329\n",
      "Train Epoch: 5 [162816/196512 (83%)]\tLoss: 0.569715\n",
      "Train Epoch: 5 [164864/196512 (84%)]\tLoss: 0.608429\n",
      "Train Epoch: 5 [166912/196512 (85%)]\tLoss: 0.576717\n",
      "Train Epoch: 5 [168960/196512 (86%)]\tLoss: 0.593397\n",
      "Train Epoch: 5 [171008/196512 (87%)]\tLoss: 0.577041\n",
      "Train Epoch: 5 [173056/196512 (88%)]\tLoss: 0.582890\n",
      "Train Epoch: 5 [175104/196512 (89%)]\tLoss: 0.573659\n",
      "Train Epoch: 5 [177152/196512 (90%)]\tLoss: 0.580298\n",
      "Train Epoch: 5 [179200/196512 (91%)]\tLoss: 0.567585\n",
      "Train Epoch: 5 [181248/196512 (92%)]\tLoss: 0.585792\n",
      "Train Epoch: 5 [183296/196512 (93%)]\tLoss: 0.593753\n",
      "Train Epoch: 5 [185344/196512 (94%)]\tLoss: 0.592453\n",
      "Train Epoch: 5 [187392/196512 (95%)]\tLoss: 0.592979\n",
      "Train Epoch: 5 [189440/196512 (96%)]\tLoss: 0.601984\n",
      "Train Epoch: 5 [191488/196512 (97%)]\tLoss: 0.576438\n",
      "Train Epoch: 5 [193536/196512 (98%)]\tLoss: 0.596899\n",
      "Train Epoch: 5 [177248/196512 (99%)]\tLoss: 0.575278\n",
      "\n",
      "Test set: Average loss: -3605.4940, Accuracy: 32798/49128 (67%)\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "alex_net = AlexNet(input_length).to(device)\n",
    "alex_optimizer = torch.optim.Adam(alex_net.parameters(), lr=0.0001)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(alex_net, device, train_dataloader, alex_optimizer, epoch)\n",
    "    test(alex_net, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1024/196512 (1%)]\tLoss: 0.698099\n",
      "Train Epoch: 1 [3072/196512 (2%)]\tLoss: 0.689714\n",
      "Train Epoch: 1 [5120/196512 (3%)]\tLoss: 0.696458\n",
      "Train Epoch: 1 [7168/196512 (4%)]\tLoss: 0.693637\n",
      "Train Epoch: 1 [9216/196512 (5%)]\tLoss: 0.692724\n",
      "Train Epoch: 1 [11264/196512 (6%)]\tLoss: 0.693445\n",
      "Train Epoch: 1 [13312/196512 (7%)]\tLoss: 0.693039\n",
      "Train Epoch: 1 [15360/196512 (8%)]\tLoss: 0.692171\n",
      "Train Epoch: 1 [17408/196512 (9%)]\tLoss: 0.692180\n",
      "Train Epoch: 1 [19456/196512 (10%)]\tLoss: 0.692757\n",
      "Train Epoch: 1 [21504/196512 (11%)]\tLoss: 0.691688\n",
      "Train Epoch: 1 [23552/196512 (12%)]\tLoss: 0.692392\n",
      "Train Epoch: 1 [25600/196512 (13%)]\tLoss: 0.691385\n",
      "Train Epoch: 1 [27648/196512 (14%)]\tLoss: 0.692133\n",
      "Train Epoch: 1 [29696/196512 (15%)]\tLoss: 0.691187\n",
      "Train Epoch: 1 [31744/196512 (16%)]\tLoss: 0.691249\n",
      "Train Epoch: 1 [33792/196512 (17%)]\tLoss: 0.690774\n",
      "Train Epoch: 1 [35840/196512 (18%)]\tLoss: 0.690767\n",
      "Train Epoch: 1 [37888/196512 (19%)]\tLoss: 0.690077\n",
      "Train Epoch: 1 [39936/196512 (20%)]\tLoss: 0.690426\n",
      "Train Epoch: 1 [41984/196512 (21%)]\tLoss: 0.690303\n",
      "Train Epoch: 1 [44032/196512 (22%)]\tLoss: 0.691501\n",
      "Train Epoch: 1 [46080/196512 (23%)]\tLoss: 0.692095\n",
      "Train Epoch: 1 [48128/196512 (24%)]\tLoss: 0.691297\n",
      "Train Epoch: 1 [50176/196512 (26%)]\tLoss: 0.690299\n",
      "Train Epoch: 1 [52224/196512 (27%)]\tLoss: 0.690173\n",
      "Train Epoch: 1 [54272/196512 (28%)]\tLoss: 0.690003\n",
      "Train Epoch: 1 [56320/196512 (29%)]\tLoss: 0.688478\n",
      "Train Epoch: 1 [58368/196512 (30%)]\tLoss: 0.689012\n",
      "Train Epoch: 1 [60416/196512 (31%)]\tLoss: 0.688633\n",
      "Train Epoch: 1 [62464/196512 (32%)]\tLoss: 0.689204\n",
      "Train Epoch: 1 [64512/196512 (33%)]\tLoss: 0.688961\n",
      "Train Epoch: 1 [66560/196512 (34%)]\tLoss: 0.688392\n",
      "Train Epoch: 1 [68608/196512 (35%)]\tLoss: 0.687375\n",
      "Train Epoch: 1 [70656/196512 (36%)]\tLoss: 0.687826\n",
      "Train Epoch: 1 [72704/196512 (37%)]\tLoss: 0.685786\n",
      "Train Epoch: 1 [74752/196512 (38%)]\tLoss: 0.686869\n",
      "Train Epoch: 1 [76800/196512 (39%)]\tLoss: 0.686072\n",
      "Train Epoch: 1 [78848/196512 (40%)]\tLoss: 0.685911\n",
      "Train Epoch: 1 [80896/196512 (41%)]\tLoss: 0.686459\n",
      "Train Epoch: 1 [82944/196512 (42%)]\tLoss: 0.686311\n",
      "Train Epoch: 1 [84992/196512 (43%)]\tLoss: 0.684910\n",
      "Train Epoch: 1 [87040/196512 (44%)]\tLoss: 0.684917\n",
      "Train Epoch: 1 [89088/196512 (45%)]\tLoss: 0.685668\n",
      "Train Epoch: 1 [91136/196512 (46%)]\tLoss: 0.684500\n",
      "Train Epoch: 1 [93184/196512 (47%)]\tLoss: 0.681222\n",
      "Train Epoch: 1 [95232/196512 (48%)]\tLoss: 0.684840\n",
      "Train Epoch: 1 [97280/196512 (49%)]\tLoss: 0.684943\n",
      "Train Epoch: 1 [99328/196512 (51%)]\tLoss: 0.681673\n",
      "Train Epoch: 1 [101376/196512 (52%)]\tLoss: 0.681592\n",
      "Train Epoch: 1 [103424/196512 (53%)]\tLoss: 0.682094\n",
      "Train Epoch: 1 [105472/196512 (54%)]\tLoss: 0.680474\n",
      "Train Epoch: 1 [107520/196512 (55%)]\tLoss: 0.681800\n",
      "Train Epoch: 1 [109568/196512 (56%)]\tLoss: 0.679944\n",
      "Train Epoch: 1 [111616/196512 (57%)]\tLoss: 0.679048\n",
      "Train Epoch: 1 [113664/196512 (58%)]\tLoss: 0.677849\n",
      "Train Epoch: 1 [115712/196512 (59%)]\tLoss: 0.677964\n",
      "Train Epoch: 1 [117760/196512 (60%)]\tLoss: 0.674915\n",
      "Train Epoch: 1 [119808/196512 (61%)]\tLoss: 0.677096\n",
      "Train Epoch: 1 [121856/196512 (62%)]\tLoss: 0.679277\n",
      "Train Epoch: 1 [123904/196512 (63%)]\tLoss: 0.681721\n",
      "Train Epoch: 1 [125952/196512 (64%)]\tLoss: 0.674809\n",
      "Train Epoch: 1 [128000/196512 (65%)]\tLoss: 0.675396\n",
      "Train Epoch: 1 [130048/196512 (66%)]\tLoss: 0.674955\n",
      "Train Epoch: 1 [132096/196512 (67%)]\tLoss: 0.675953\n",
      "Train Epoch: 1 [134144/196512 (68%)]\tLoss: 0.674070\n",
      "Train Epoch: 1 [136192/196512 (69%)]\tLoss: 0.672217\n",
      "Train Epoch: 1 [138240/196512 (70%)]\tLoss: 0.669589\n",
      "Train Epoch: 1 [140288/196512 (71%)]\tLoss: 0.669706\n",
      "Train Epoch: 1 [142336/196512 (72%)]\tLoss: 0.670739\n",
      "Train Epoch: 1 [144384/196512 (73%)]\tLoss: 0.668116\n",
      "Train Epoch: 1 [146432/196512 (74%)]\tLoss: 0.667667\n",
      "Train Epoch: 1 [148480/196512 (76%)]\tLoss: 0.666674\n",
      "Train Epoch: 1 [150528/196512 (77%)]\tLoss: 0.668911\n",
      "Train Epoch: 1 [152576/196512 (78%)]\tLoss: 0.664811\n",
      "Train Epoch: 1 [154624/196512 (79%)]\tLoss: 0.666944\n",
      "Train Epoch: 1 [156672/196512 (80%)]\tLoss: 0.662622\n",
      "Train Epoch: 1 [158720/196512 (81%)]\tLoss: 0.660042\n",
      "Train Epoch: 1 [160768/196512 (82%)]\tLoss: 0.657890\n",
      "Train Epoch: 1 [162816/196512 (83%)]\tLoss: 0.671854\n",
      "Train Epoch: 1 [164864/196512 (84%)]\tLoss: 0.663109\n",
      "Train Epoch: 1 [166912/196512 (85%)]\tLoss: 0.657329\n",
      "Train Epoch: 1 [168960/196512 (86%)]\tLoss: 0.659267\n",
      "Train Epoch: 1 [171008/196512 (87%)]\tLoss: 0.658345\n",
      "Train Epoch: 1 [173056/196512 (88%)]\tLoss: 0.644847\n",
      "Train Epoch: 1 [175104/196512 (89%)]\tLoss: 0.653909\n",
      "Train Epoch: 1 [177152/196512 (90%)]\tLoss: 0.644914\n",
      "Train Epoch: 1 [179200/196512 (91%)]\tLoss: 0.657785\n",
      "Train Epoch: 1 [181248/196512 (92%)]\tLoss: 0.661631\n",
      "Train Epoch: 1 [183296/196512 (93%)]\tLoss: 0.646097\n",
      "Train Epoch: 1 [185344/196512 (94%)]\tLoss: 0.635902\n",
      "Train Epoch: 1 [187392/196512 (95%)]\tLoss: 0.660872\n",
      "Train Epoch: 1 [189440/196512 (96%)]\tLoss: 0.654826\n",
      "Train Epoch: 1 [191488/196512 (97%)]\tLoss: 0.640689\n",
      "Train Epoch: 1 [193536/196512 (98%)]\tLoss: 0.654331\n",
      "Train Epoch: 1 [177248/196512 (99%)]\tLoss: 0.635223\n",
      "\n",
      "Test set: Average loss: -58.6825, Accuracy: 30964/49128 (63%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 2 [1024/196512 (1%)]\tLoss: 0.651118\n",
      "Train Epoch: 2 [3072/196512 (2%)]\tLoss: 0.636841\n",
      "Train Epoch: 2 [5120/196512 (3%)]\tLoss: 0.638888\n",
      "Train Epoch: 2 [7168/196512 (4%)]\tLoss: 0.652090\n",
      "Train Epoch: 2 [9216/196512 (5%)]\tLoss: 0.629169\n",
      "Train Epoch: 2 [11264/196512 (6%)]\tLoss: 0.641076\n",
      "Train Epoch: 2 [13312/196512 (7%)]\tLoss: 0.643229\n",
      "Train Epoch: 2 [15360/196512 (8%)]\tLoss: 0.639154\n",
      "Train Epoch: 2 [17408/196512 (9%)]\tLoss: 0.647532\n",
      "Train Epoch: 2 [19456/196512 (10%)]\tLoss: 0.633026\n",
      "Train Epoch: 2 [21504/196512 (11%)]\tLoss: 0.637203\n",
      "Train Epoch: 2 [23552/196512 (12%)]\tLoss: 0.633410\n",
      "Train Epoch: 2 [25600/196512 (13%)]\tLoss: 0.642503\n",
      "Train Epoch: 2 [27648/196512 (14%)]\tLoss: 0.633013\n",
      "Train Epoch: 2 [29696/196512 (15%)]\tLoss: 0.648507\n",
      "Train Epoch: 2 [31744/196512 (16%)]\tLoss: 0.643530\n",
      "Train Epoch: 2 [33792/196512 (17%)]\tLoss: 0.638071\n",
      "Train Epoch: 2 [35840/196512 (18%)]\tLoss: 0.633028\n",
      "Train Epoch: 2 [37888/196512 (19%)]\tLoss: 0.637419\n",
      "Train Epoch: 2 [39936/196512 (20%)]\tLoss: 0.644824\n",
      "Train Epoch: 2 [41984/196512 (21%)]\tLoss: 0.661043\n",
      "Train Epoch: 2 [44032/196512 (22%)]\tLoss: 0.624319\n",
      "Train Epoch: 2 [46080/196512 (23%)]\tLoss: 0.636280\n",
      "Train Epoch: 2 [48128/196512 (24%)]\tLoss: 0.627977\n",
      "Train Epoch: 2 [50176/196512 (26%)]\tLoss: 0.632778\n",
      "Train Epoch: 2 [52224/196512 (27%)]\tLoss: 0.634240\n",
      "Train Epoch: 2 [54272/196512 (28%)]\tLoss: 0.631262\n",
      "Train Epoch: 2 [56320/196512 (29%)]\tLoss: 0.652742\n",
      "Train Epoch: 2 [58368/196512 (30%)]\tLoss: 0.632815\n",
      "Train Epoch: 2 [60416/196512 (31%)]\tLoss: 0.646734\n",
      "Train Epoch: 2 [62464/196512 (32%)]\tLoss: 0.630730\n",
      "Train Epoch: 2 [64512/196512 (33%)]\tLoss: 0.642865\n",
      "Train Epoch: 2 [66560/196512 (34%)]\tLoss: 0.643696\n",
      "Train Epoch: 2 [68608/196512 (35%)]\tLoss: 0.640893\n",
      "Train Epoch: 2 [70656/196512 (36%)]\tLoss: 0.645359\n",
      "Train Epoch: 2 [72704/196512 (37%)]\tLoss: 0.641037\n",
      "Train Epoch: 2 [74752/196512 (38%)]\tLoss: 0.639230\n",
      "Train Epoch: 2 [76800/196512 (39%)]\tLoss: 0.633203\n",
      "Train Epoch: 2 [78848/196512 (40%)]\tLoss: 0.638545\n",
      "Train Epoch: 2 [80896/196512 (41%)]\tLoss: 0.622676\n",
      "Train Epoch: 2 [82944/196512 (42%)]\tLoss: 0.643317\n",
      "Train Epoch: 2 [84992/196512 (43%)]\tLoss: 0.637629\n",
      "Train Epoch: 2 [87040/196512 (44%)]\tLoss: 0.621537\n",
      "Train Epoch: 2 [89088/196512 (45%)]\tLoss: 0.627089\n",
      "Train Epoch: 2 [91136/196512 (46%)]\tLoss: 0.636706\n",
      "Train Epoch: 2 [93184/196512 (47%)]\tLoss: 0.627857\n",
      "Train Epoch: 2 [95232/196512 (48%)]\tLoss: 0.625834\n",
      "Train Epoch: 2 [97280/196512 (49%)]\tLoss: 0.614071\n",
      "Train Epoch: 2 [99328/196512 (51%)]\tLoss: 0.653151\n",
      "Train Epoch: 2 [101376/196512 (52%)]\tLoss: 0.622365\n",
      "Train Epoch: 2 [103424/196512 (53%)]\tLoss: 0.641454\n",
      "Train Epoch: 2 [105472/196512 (54%)]\tLoss: 0.631411\n",
      "Train Epoch: 2 [107520/196512 (55%)]\tLoss: 0.650997\n",
      "Train Epoch: 2 [109568/196512 (56%)]\tLoss: 0.619131\n",
      "Train Epoch: 2 [111616/196512 (57%)]\tLoss: 0.626353\n",
      "Train Epoch: 2 [113664/196512 (58%)]\tLoss: 0.629931\n",
      "Train Epoch: 2 [115712/196512 (59%)]\tLoss: 0.625400\n",
      "Train Epoch: 2 [117760/196512 (60%)]\tLoss: 0.645532\n",
      "Train Epoch: 2 [119808/196512 (61%)]\tLoss: 0.651863\n",
      "Train Epoch: 2 [121856/196512 (62%)]\tLoss: 0.635363\n",
      "Train Epoch: 2 [123904/196512 (63%)]\tLoss: 0.621610\n",
      "Train Epoch: 2 [125952/196512 (64%)]\tLoss: 0.638797\n",
      "Train Epoch: 2 [128000/196512 (65%)]\tLoss: 0.630691\n",
      "Train Epoch: 2 [130048/196512 (66%)]\tLoss: 0.631454\n",
      "Train Epoch: 2 [132096/196512 (67%)]\tLoss: 0.630550\n",
      "Train Epoch: 2 [134144/196512 (68%)]\tLoss: 0.634498\n",
      "Train Epoch: 2 [136192/196512 (69%)]\tLoss: 0.628355\n",
      "Train Epoch: 2 [138240/196512 (70%)]\tLoss: 0.642180\n",
      "Train Epoch: 2 [140288/196512 (71%)]\tLoss: 0.644189\n",
      "Train Epoch: 2 [142336/196512 (72%)]\tLoss: 0.638318\n",
      "Train Epoch: 2 [144384/196512 (73%)]\tLoss: 0.648912\n",
      "Train Epoch: 2 [146432/196512 (74%)]\tLoss: 0.622089\n",
      "Train Epoch: 2 [148480/196512 (76%)]\tLoss: 0.636516\n",
      "Train Epoch: 2 [150528/196512 (77%)]\tLoss: 0.630075\n",
      "Train Epoch: 2 [152576/196512 (78%)]\tLoss: 0.634698\n",
      "Train Epoch: 2 [154624/196512 (79%)]\tLoss: 0.625938\n",
      "Train Epoch: 2 [156672/196512 (80%)]\tLoss: 0.634501\n",
      "Train Epoch: 2 [158720/196512 (81%)]\tLoss: 0.623730\n",
      "Train Epoch: 2 [160768/196512 (82%)]\tLoss: 0.644705\n",
      "Train Epoch: 2 [162816/196512 (83%)]\tLoss: 0.643955\n",
      "Train Epoch: 2 [164864/196512 (84%)]\tLoss: 0.637642\n",
      "Train Epoch: 2 [166912/196512 (85%)]\tLoss: 0.644257\n",
      "Train Epoch: 2 [168960/196512 (86%)]\tLoss: 0.635975\n",
      "Train Epoch: 2 [171008/196512 (87%)]\tLoss: 0.625612\n",
      "Train Epoch: 2 [173056/196512 (88%)]\tLoss: 0.627490\n",
      "Train Epoch: 2 [175104/196512 (89%)]\tLoss: 0.643863\n",
      "Train Epoch: 2 [177152/196512 (90%)]\tLoss: 0.632179\n",
      "Train Epoch: 2 [179200/196512 (91%)]\tLoss: 0.629797\n",
      "Train Epoch: 2 [181248/196512 (92%)]\tLoss: 0.620021\n",
      "Train Epoch: 2 [183296/196512 (93%)]\tLoss: 0.648430\n",
      "Train Epoch: 2 [185344/196512 (94%)]\tLoss: 0.627805\n",
      "Train Epoch: 2 [187392/196512 (95%)]\tLoss: 0.627413\n",
      "Train Epoch: 2 [189440/196512 (96%)]\tLoss: 0.637824\n",
      "Train Epoch: 2 [191488/196512 (97%)]\tLoss: 0.634372\n",
      "Train Epoch: 2 [193536/196512 (98%)]\tLoss: 0.642372\n",
      "Train Epoch: 2 [177248/196512 (99%)]\tLoss: 0.622880\n",
      "\n",
      "Test set: Average loss: -101.8478, Accuracy: 31311/49128 (64%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 3 [1024/196512 (1%)]\tLoss: 0.618865\n",
      "Train Epoch: 3 [3072/196512 (2%)]\tLoss: 0.638547\n",
      "Train Epoch: 3 [5120/196512 (3%)]\tLoss: 0.647405\n",
      "Train Epoch: 3 [7168/196512 (4%)]\tLoss: 0.617224\n",
      "Train Epoch: 3 [9216/196512 (5%)]\tLoss: 0.628993\n",
      "Train Epoch: 3 [11264/196512 (6%)]\tLoss: 0.640953\n",
      "Train Epoch: 3 [13312/196512 (7%)]\tLoss: 0.640528\n",
      "Train Epoch: 3 [15360/196512 (8%)]\tLoss: 0.633879\n",
      "Train Epoch: 3 [17408/196512 (9%)]\tLoss: 0.635498\n",
      "Train Epoch: 3 [19456/196512 (10%)]\tLoss: 0.644821\n",
      "Train Epoch: 3 [21504/196512 (11%)]\tLoss: 0.654812\n",
      "Train Epoch: 3 [23552/196512 (12%)]\tLoss: 0.630773\n",
      "Train Epoch: 3 [25600/196512 (13%)]\tLoss: 0.620768\n",
      "Train Epoch: 3 [27648/196512 (14%)]\tLoss: 0.630006\n",
      "Train Epoch: 3 [29696/196512 (15%)]\tLoss: 0.630826\n",
      "Train Epoch: 3 [31744/196512 (16%)]\tLoss: 0.611663\n",
      "Train Epoch: 3 [33792/196512 (17%)]\tLoss: 0.622075\n",
      "Train Epoch: 3 [35840/196512 (18%)]\tLoss: 0.631189\n",
      "Train Epoch: 3 [37888/196512 (19%)]\tLoss: 0.638422\n",
      "Train Epoch: 3 [39936/196512 (20%)]\tLoss: 0.609573\n",
      "Train Epoch: 3 [41984/196512 (21%)]\tLoss: 0.648059\n",
      "Train Epoch: 3 [44032/196512 (22%)]\tLoss: 0.640295\n",
      "Train Epoch: 3 [46080/196512 (23%)]\tLoss: 0.637471\n",
      "Train Epoch: 3 [48128/196512 (24%)]\tLoss: 0.631496\n",
      "Train Epoch: 3 [50176/196512 (26%)]\tLoss: 0.621117\n",
      "Train Epoch: 3 [52224/196512 (27%)]\tLoss: 0.634160\n",
      "Train Epoch: 3 [54272/196512 (28%)]\tLoss: 0.630132\n",
      "Train Epoch: 3 [56320/196512 (29%)]\tLoss: 0.622883\n",
      "Train Epoch: 3 [58368/196512 (30%)]\tLoss: 0.631576\n",
      "Train Epoch: 3 [60416/196512 (31%)]\tLoss: 0.639221\n",
      "Train Epoch: 3 [62464/196512 (32%)]\tLoss: 0.618648\n",
      "Train Epoch: 3 [64512/196512 (33%)]\tLoss: 0.630209\n",
      "Train Epoch: 3 [66560/196512 (34%)]\tLoss: 0.636161\n",
      "Train Epoch: 3 [68608/196512 (35%)]\tLoss: 0.632369\n",
      "Train Epoch: 3 [70656/196512 (36%)]\tLoss: 0.626417\n",
      "Train Epoch: 3 [72704/196512 (37%)]\tLoss: 0.632121\n",
      "Train Epoch: 3 [74752/196512 (38%)]\tLoss: 0.617573\n",
      "Train Epoch: 3 [76800/196512 (39%)]\tLoss: 0.622972\n",
      "Train Epoch: 3 [78848/196512 (40%)]\tLoss: 0.617785\n",
      "Train Epoch: 3 [80896/196512 (41%)]\tLoss: 0.624977\n",
      "Train Epoch: 3 [82944/196512 (42%)]\tLoss: 0.618687\n",
      "Train Epoch: 3 [84992/196512 (43%)]\tLoss: 0.627178\n",
      "Train Epoch: 3 [87040/196512 (44%)]\tLoss: 0.632016\n",
      "Train Epoch: 3 [89088/196512 (45%)]\tLoss: 0.638220\n",
      "Train Epoch: 3 [91136/196512 (46%)]\tLoss: 0.642164\n",
      "Train Epoch: 3 [93184/196512 (47%)]\tLoss: 0.616248\n",
      "Train Epoch: 3 [95232/196512 (48%)]\tLoss: 0.641763\n",
      "Train Epoch: 3 [97280/196512 (49%)]\tLoss: 0.649993\n",
      "Train Epoch: 3 [99328/196512 (51%)]\tLoss: 0.637528\n",
      "Train Epoch: 3 [101376/196512 (52%)]\tLoss: 0.635159\n",
      "Train Epoch: 3 [103424/196512 (53%)]\tLoss: 0.632195\n",
      "Train Epoch: 3 [105472/196512 (54%)]\tLoss: 0.636836\n",
      "Train Epoch: 3 [107520/196512 (55%)]\tLoss: 0.632677\n",
      "Train Epoch: 3 [109568/196512 (56%)]\tLoss: 0.615332\n",
      "Train Epoch: 3 [111616/196512 (57%)]\tLoss: 0.627839\n",
      "Train Epoch: 3 [113664/196512 (58%)]\tLoss: 0.628446\n",
      "Train Epoch: 3 [115712/196512 (59%)]\tLoss: 0.633120\n",
      "Train Epoch: 3 [117760/196512 (60%)]\tLoss: 0.630339\n",
      "Train Epoch: 3 [119808/196512 (61%)]\tLoss: 0.630610\n",
      "Train Epoch: 3 [121856/196512 (62%)]\tLoss: 0.633805\n",
      "Train Epoch: 3 [123904/196512 (63%)]\tLoss: 0.640487\n",
      "Train Epoch: 3 [125952/196512 (64%)]\tLoss: 0.625601\n",
      "Train Epoch: 3 [128000/196512 (65%)]\tLoss: 0.622292\n",
      "Train Epoch: 3 [130048/196512 (66%)]\tLoss: 0.620599\n",
      "Train Epoch: 3 [132096/196512 (67%)]\tLoss: 0.621832\n",
      "Train Epoch: 3 [134144/196512 (68%)]\tLoss: 0.641247\n",
      "Train Epoch: 3 [136192/196512 (69%)]\tLoss: 0.621066\n",
      "Train Epoch: 3 [138240/196512 (70%)]\tLoss: 0.635997\n",
      "Train Epoch: 3 [140288/196512 (71%)]\tLoss: 0.618803\n",
      "Train Epoch: 3 [142336/196512 (72%)]\tLoss: 0.625541\n",
      "Train Epoch: 3 [144384/196512 (73%)]\tLoss: 0.633520\n",
      "Train Epoch: 3 [146432/196512 (74%)]\tLoss: 0.623937\n",
      "Train Epoch: 3 [148480/196512 (76%)]\tLoss: 0.628794\n",
      "Train Epoch: 3 [150528/196512 (77%)]\tLoss: 0.613418\n",
      "Train Epoch: 3 [152576/196512 (78%)]\tLoss: 0.647362\n",
      "Train Epoch: 3 [154624/196512 (79%)]\tLoss: 0.607639\n",
      "Train Epoch: 3 [156672/196512 (80%)]\tLoss: 0.631745\n",
      "Train Epoch: 3 [158720/196512 (81%)]\tLoss: 0.634743\n",
      "Train Epoch: 3 [160768/196512 (82%)]\tLoss: 0.632593\n",
      "Train Epoch: 3 [162816/196512 (83%)]\tLoss: 0.624522\n",
      "Train Epoch: 3 [164864/196512 (84%)]\tLoss: 0.624807\n",
      "Train Epoch: 3 [166912/196512 (85%)]\tLoss: 0.621334\n",
      "Train Epoch: 3 [168960/196512 (86%)]\tLoss: 0.633549\n",
      "Train Epoch: 3 [171008/196512 (87%)]\tLoss: 0.629038\n",
      "Train Epoch: 3 [173056/196512 (88%)]\tLoss: 0.644450\n",
      "Train Epoch: 3 [175104/196512 (89%)]\tLoss: 0.621288\n",
      "Train Epoch: 3 [177152/196512 (90%)]\tLoss: 0.623807\n",
      "Train Epoch: 3 [179200/196512 (91%)]\tLoss: 0.608382\n",
      "Train Epoch: 3 [181248/196512 (92%)]\tLoss: 0.643045\n",
      "Train Epoch: 3 [183296/196512 (93%)]\tLoss: 0.614158\n",
      "Train Epoch: 3 [185344/196512 (94%)]\tLoss: 0.634213\n",
      "Train Epoch: 3 [187392/196512 (95%)]\tLoss: 0.632923\n",
      "Train Epoch: 3 [189440/196512 (96%)]\tLoss: 0.629212\n",
      "Train Epoch: 3 [191488/196512 (97%)]\tLoss: 0.625485\n",
      "Train Epoch: 3 [193536/196512 (98%)]\tLoss: 0.625678\n",
      "Train Epoch: 3 [177248/196512 (99%)]\tLoss: 0.626471\n",
      "\n",
      "Test set: Average loss: -131.5284, Accuracy: 31514/49128 (64%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 4 [1024/196512 (1%)]\tLoss: 0.636353\n",
      "Train Epoch: 4 [3072/196512 (2%)]\tLoss: 0.619992\n",
      "Train Epoch: 4 [5120/196512 (3%)]\tLoss: 0.616226\n",
      "Train Epoch: 4 [7168/196512 (4%)]\tLoss: 0.616253\n",
      "Train Epoch: 4 [9216/196512 (5%)]\tLoss: 0.620169\n",
      "Train Epoch: 4 [11264/196512 (6%)]\tLoss: 0.631173\n",
      "Train Epoch: 4 [13312/196512 (7%)]\tLoss: 0.628100\n",
      "Train Epoch: 4 [15360/196512 (8%)]\tLoss: 0.642335\n",
      "Train Epoch: 4 [17408/196512 (9%)]\tLoss: 0.631126\n",
      "Train Epoch: 4 [19456/196512 (10%)]\tLoss: 0.621136\n",
      "Train Epoch: 4 [21504/196512 (11%)]\tLoss: 0.633077\n",
      "Train Epoch: 4 [23552/196512 (12%)]\tLoss: 0.619749\n",
      "Train Epoch: 4 [25600/196512 (13%)]\tLoss: 0.631253\n",
      "Train Epoch: 4 [27648/196512 (14%)]\tLoss: 0.619922\n",
      "Train Epoch: 4 [29696/196512 (15%)]\tLoss: 0.616631\n",
      "Train Epoch: 4 [31744/196512 (16%)]\tLoss: 0.624632\n",
      "Train Epoch: 4 [33792/196512 (17%)]\tLoss: 0.639877\n",
      "Train Epoch: 4 [35840/196512 (18%)]\tLoss: 0.606786\n",
      "Train Epoch: 4 [37888/196512 (19%)]\tLoss: 0.603408\n",
      "Train Epoch: 4 [39936/196512 (20%)]\tLoss: 0.621705\n",
      "Train Epoch: 4 [41984/196512 (21%)]\tLoss: 0.617279\n",
      "Train Epoch: 4 [44032/196512 (22%)]\tLoss: 0.646173\n",
      "Train Epoch: 4 [46080/196512 (23%)]\tLoss: 0.609612\n",
      "Train Epoch: 4 [48128/196512 (24%)]\tLoss: 0.619199\n",
      "Train Epoch: 4 [50176/196512 (26%)]\tLoss: 0.616602\n",
      "Train Epoch: 4 [52224/196512 (27%)]\tLoss: 0.638332\n",
      "Train Epoch: 4 [54272/196512 (28%)]\tLoss: 0.626336\n",
      "Train Epoch: 4 [56320/196512 (29%)]\tLoss: 0.608640\n",
      "Train Epoch: 4 [58368/196512 (30%)]\tLoss: 0.613768\n",
      "Train Epoch: 4 [60416/196512 (31%)]\tLoss: 0.616559\n",
      "Train Epoch: 4 [62464/196512 (32%)]\tLoss: 0.625407\n",
      "Train Epoch: 4 [64512/196512 (33%)]\tLoss: 0.644269\n",
      "Train Epoch: 4 [66560/196512 (34%)]\tLoss: 0.635645\n",
      "Train Epoch: 4 [68608/196512 (35%)]\tLoss: 0.625855\n",
      "Train Epoch: 4 [70656/196512 (36%)]\tLoss: 0.629004\n",
      "Train Epoch: 4 [72704/196512 (37%)]\tLoss: 0.631460\n",
      "Train Epoch: 4 [74752/196512 (38%)]\tLoss: 0.613727\n",
      "Train Epoch: 4 [76800/196512 (39%)]\tLoss: 0.641648\n",
      "Train Epoch: 4 [78848/196512 (40%)]\tLoss: 0.624740\n",
      "Train Epoch: 4 [80896/196512 (41%)]\tLoss: 0.640718\n",
      "Train Epoch: 4 [82944/196512 (42%)]\tLoss: 0.634190\n",
      "Train Epoch: 4 [84992/196512 (43%)]\tLoss: 0.617772\n",
      "Train Epoch: 4 [87040/196512 (44%)]\tLoss: 0.611711\n",
      "Train Epoch: 4 [89088/196512 (45%)]\tLoss: 0.612753\n",
      "Train Epoch: 4 [91136/196512 (46%)]\tLoss: 0.631861\n",
      "Train Epoch: 4 [93184/196512 (47%)]\tLoss: 0.631700\n",
      "Train Epoch: 4 [95232/196512 (48%)]\tLoss: 0.619537\n",
      "Train Epoch: 4 [97280/196512 (49%)]\tLoss: 0.613872\n",
      "Train Epoch: 4 [99328/196512 (51%)]\tLoss: 0.629597\n",
      "Train Epoch: 4 [101376/196512 (52%)]\tLoss: 0.601756\n",
      "Train Epoch: 4 [103424/196512 (53%)]\tLoss: 0.638990\n",
      "Train Epoch: 4 [105472/196512 (54%)]\tLoss: 0.635141\n",
      "Train Epoch: 4 [107520/196512 (55%)]\tLoss: 0.621182\n",
      "Train Epoch: 4 [109568/196512 (56%)]\tLoss: 0.633198\n",
      "Train Epoch: 4 [111616/196512 (57%)]\tLoss: 0.617793\n",
      "Train Epoch: 4 [113664/196512 (58%)]\tLoss: 0.612020\n",
      "Train Epoch: 4 [115712/196512 (59%)]\tLoss: 0.629783\n",
      "Train Epoch: 4 [117760/196512 (60%)]\tLoss: 0.637541\n",
      "Train Epoch: 4 [119808/196512 (61%)]\tLoss: 0.641631\n",
      "Train Epoch: 4 [121856/196512 (62%)]\tLoss: 0.623910\n",
      "Train Epoch: 4 [123904/196512 (63%)]\tLoss: 0.612373\n",
      "Train Epoch: 4 [125952/196512 (64%)]\tLoss: 0.618144\n",
      "Train Epoch: 4 [128000/196512 (65%)]\tLoss: 0.625250\n",
      "Train Epoch: 4 [130048/196512 (66%)]\tLoss: 0.635123\n",
      "Train Epoch: 4 [132096/196512 (67%)]\tLoss: 0.618937\n",
      "Train Epoch: 4 [134144/196512 (68%)]\tLoss: 0.622749\n",
      "Train Epoch: 4 [136192/196512 (69%)]\tLoss: 0.622869\n",
      "Train Epoch: 4 [138240/196512 (70%)]\tLoss: 0.625984\n",
      "Train Epoch: 4 [140288/196512 (71%)]\tLoss: 0.614742\n",
      "Train Epoch: 4 [142336/196512 (72%)]\tLoss: 0.642881\n",
      "Train Epoch: 4 [144384/196512 (73%)]\tLoss: 0.607215\n",
      "Train Epoch: 4 [146432/196512 (74%)]\tLoss: 0.616465\n",
      "Train Epoch: 4 [148480/196512 (76%)]\tLoss: 0.628559\n",
      "Train Epoch: 4 [150528/196512 (77%)]\tLoss: 0.624678\n",
      "Train Epoch: 4 [152576/196512 (78%)]\tLoss: 0.617747\n",
      "Train Epoch: 4 [154624/196512 (79%)]\tLoss: 0.615668\n",
      "Train Epoch: 4 [156672/196512 (80%)]\tLoss: 0.623245\n",
      "Train Epoch: 4 [158720/196512 (81%)]\tLoss: 0.627676\n",
      "Train Epoch: 4 [160768/196512 (82%)]\tLoss: 0.617421\n",
      "Train Epoch: 4 [162816/196512 (83%)]\tLoss: 0.616174\n",
      "Train Epoch: 4 [164864/196512 (84%)]\tLoss: 0.629623\n",
      "Train Epoch: 4 [166912/196512 (85%)]\tLoss: 0.626345\n",
      "Train Epoch: 4 [168960/196512 (86%)]\tLoss: 0.612992\n",
      "Train Epoch: 4 [171008/196512 (87%)]\tLoss: 0.620976\n",
      "Train Epoch: 4 [173056/196512 (88%)]\tLoss: 0.642870\n",
      "Train Epoch: 4 [175104/196512 (89%)]\tLoss: 0.621491\n",
      "Train Epoch: 4 [177152/196512 (90%)]\tLoss: 0.622969\n",
      "Train Epoch: 4 [179200/196512 (91%)]\tLoss: 0.625479\n",
      "Train Epoch: 4 [181248/196512 (92%)]\tLoss: 0.626452\n",
      "Train Epoch: 4 [183296/196512 (93%)]\tLoss: 0.626918\n",
      "Train Epoch: 4 [185344/196512 (94%)]\tLoss: 0.633507\n",
      "Train Epoch: 4 [187392/196512 (95%)]\tLoss: 0.620534\n",
      "Train Epoch: 4 [189440/196512 (96%)]\tLoss: 0.610626\n",
      "Train Epoch: 4 [191488/196512 (97%)]\tLoss: 0.637076\n",
      "Train Epoch: 4 [193536/196512 (98%)]\tLoss: 0.633517\n",
      "Train Epoch: 4 [177248/196512 (99%)]\tLoss: 0.612920\n",
      "\n",
      "Test set: Average loss: -156.3552, Accuracy: 31698/49128 (65%)\n",
      "\n",
      "==============================\n",
      "Train Epoch: 5 [1024/196512 (1%)]\tLoss: 0.611023\n",
      "Train Epoch: 5 [3072/196512 (2%)]\tLoss: 0.637151\n",
      "Train Epoch: 5 [5120/196512 (3%)]\tLoss: 0.609879\n",
      "Train Epoch: 5 [7168/196512 (4%)]\tLoss: 0.623460\n",
      "Train Epoch: 5 [9216/196512 (5%)]\tLoss: 0.623400\n",
      "Train Epoch: 5 [11264/196512 (6%)]\tLoss: 0.607784\n",
      "Train Epoch: 5 [13312/196512 (7%)]\tLoss: 0.612920\n",
      "Train Epoch: 5 [15360/196512 (8%)]\tLoss: 0.615905\n",
      "Train Epoch: 5 [17408/196512 (9%)]\tLoss: 0.618127\n",
      "Train Epoch: 5 [19456/196512 (10%)]\tLoss: 0.631907\n",
      "Train Epoch: 5 [21504/196512 (11%)]\tLoss: 0.619386\n",
      "Train Epoch: 5 [23552/196512 (12%)]\tLoss: 0.619719\n",
      "Train Epoch: 5 [25600/196512 (13%)]\tLoss: 0.624292\n",
      "Train Epoch: 5 [27648/196512 (14%)]\tLoss: 0.629086\n",
      "Train Epoch: 5 [29696/196512 (15%)]\tLoss: 0.621117\n",
      "Train Epoch: 5 [31744/196512 (16%)]\tLoss: 0.621577\n",
      "Train Epoch: 5 [33792/196512 (17%)]\tLoss: 0.602684\n",
      "Train Epoch: 5 [35840/196512 (18%)]\tLoss: 0.602851\n",
      "Train Epoch: 5 [37888/196512 (19%)]\tLoss: 0.633605\n",
      "Train Epoch: 5 [39936/196512 (20%)]\tLoss: 0.623656\n",
      "Train Epoch: 5 [41984/196512 (21%)]\tLoss: 0.635417\n",
      "Train Epoch: 5 [44032/196512 (22%)]\tLoss: 0.627106\n",
      "Train Epoch: 5 [46080/196512 (23%)]\tLoss: 0.623716\n",
      "Train Epoch: 5 [48128/196512 (24%)]\tLoss: 0.618988\n",
      "Train Epoch: 5 [50176/196512 (26%)]\tLoss: 0.633577\n",
      "Train Epoch: 5 [52224/196512 (27%)]\tLoss: 0.634721\n",
      "Train Epoch: 5 [54272/196512 (28%)]\tLoss: 0.636225\n",
      "Train Epoch: 5 [56320/196512 (29%)]\tLoss: 0.610639\n",
      "Train Epoch: 5 [58368/196512 (30%)]\tLoss: 0.608518\n",
      "Train Epoch: 5 [60416/196512 (31%)]\tLoss: 0.607259\n",
      "Train Epoch: 5 [62464/196512 (32%)]\tLoss: 0.615081\n",
      "Train Epoch: 5 [64512/196512 (33%)]\tLoss: 0.613390\n",
      "Train Epoch: 5 [66560/196512 (34%)]\tLoss: 0.613071\n",
      "Train Epoch: 5 [68608/196512 (35%)]\tLoss: 0.617457\n",
      "Train Epoch: 5 [70656/196512 (36%)]\tLoss: 0.637050\n",
      "Train Epoch: 5 [72704/196512 (37%)]\tLoss: 0.605709\n",
      "Train Epoch: 5 [74752/196512 (38%)]\tLoss: 0.623157\n",
      "Train Epoch: 5 [76800/196512 (39%)]\tLoss: 0.607717\n",
      "Train Epoch: 5 [78848/196512 (40%)]\tLoss: 0.606459\n",
      "Train Epoch: 5 [80896/196512 (41%)]\tLoss: 0.611528\n",
      "Train Epoch: 5 [82944/196512 (42%)]\tLoss: 0.617182\n",
      "Train Epoch: 5 [84992/196512 (43%)]\tLoss: 0.632993\n",
      "Train Epoch: 5 [87040/196512 (44%)]\tLoss: 0.609204\n",
      "Train Epoch: 5 [89088/196512 (45%)]\tLoss: 0.614821\n",
      "Train Epoch: 5 [91136/196512 (46%)]\tLoss: 0.625345\n",
      "Train Epoch: 5 [93184/196512 (47%)]\tLoss: 0.608559\n",
      "Train Epoch: 5 [95232/196512 (48%)]\tLoss: 0.615833\n",
      "Train Epoch: 5 [97280/196512 (49%)]\tLoss: 0.624560\n",
      "Train Epoch: 5 [99328/196512 (51%)]\tLoss: 0.618911\n",
      "Train Epoch: 5 [101376/196512 (52%)]\tLoss: 0.589217\n",
      "Train Epoch: 5 [103424/196512 (53%)]\tLoss: 0.619863\n",
      "Train Epoch: 5 [105472/196512 (54%)]\tLoss: 0.619135\n",
      "Train Epoch: 5 [107520/196512 (55%)]\tLoss: 0.625800\n",
      "Train Epoch: 5 [109568/196512 (56%)]\tLoss: 0.617108\n",
      "Train Epoch: 5 [111616/196512 (57%)]\tLoss: 0.636073\n",
      "Train Epoch: 5 [113664/196512 (58%)]\tLoss: 0.636971\n",
      "Train Epoch: 5 [115712/196512 (59%)]\tLoss: 0.629126\n",
      "Train Epoch: 5 [117760/196512 (60%)]\tLoss: 0.628458\n",
      "Train Epoch: 5 [119808/196512 (61%)]\tLoss: 0.631379\n",
      "Train Epoch: 5 [121856/196512 (62%)]\tLoss: 0.620030\n",
      "Train Epoch: 5 [123904/196512 (63%)]\tLoss: 0.628443\n",
      "Train Epoch: 5 [125952/196512 (64%)]\tLoss: 0.647823\n",
      "Train Epoch: 5 [128000/196512 (65%)]\tLoss: 0.617254\n",
      "Train Epoch: 5 [130048/196512 (66%)]\tLoss: 0.609005\n",
      "Train Epoch: 5 [132096/196512 (67%)]\tLoss: 0.629116\n",
      "Train Epoch: 5 [134144/196512 (68%)]\tLoss: 0.608383\n",
      "Train Epoch: 5 [136192/196512 (69%)]\tLoss: 0.615043\n",
      "Train Epoch: 5 [138240/196512 (70%)]\tLoss: 0.614452\n",
      "Train Epoch: 5 [140288/196512 (71%)]\tLoss: 0.618620\n",
      "Train Epoch: 5 [142336/196512 (72%)]\tLoss: 0.608603\n",
      "Train Epoch: 5 [144384/196512 (73%)]\tLoss: 0.625715\n",
      "Train Epoch: 5 [146432/196512 (74%)]\tLoss: 0.614958\n",
      "Train Epoch: 5 [148480/196512 (76%)]\tLoss: 0.618293\n",
      "Train Epoch: 5 [150528/196512 (77%)]\tLoss: 0.614117\n",
      "Train Epoch: 5 [152576/196512 (78%)]\tLoss: 0.612735\n",
      "Train Epoch: 5 [154624/196512 (79%)]\tLoss: 0.627039\n",
      "Train Epoch: 5 [156672/196512 (80%)]\tLoss: 0.627778\n",
      "Train Epoch: 5 [158720/196512 (81%)]\tLoss: 0.624663\n",
      "Train Epoch: 5 [160768/196512 (82%)]\tLoss: 0.623423\n",
      "Train Epoch: 5 [162816/196512 (83%)]\tLoss: 0.614479\n",
      "Train Epoch: 5 [164864/196512 (84%)]\tLoss: 0.605159\n",
      "Train Epoch: 5 [166912/196512 (85%)]\tLoss: 0.593579\n",
      "Train Epoch: 5 [168960/196512 (86%)]\tLoss: 0.613888\n",
      "Train Epoch: 5 [171008/196512 (87%)]\tLoss: 0.607899\n",
      "Train Epoch: 5 [173056/196512 (88%)]\tLoss: 0.603365\n",
      "Train Epoch: 5 [175104/196512 (89%)]\tLoss: 0.612758\n",
      "Train Epoch: 5 [177152/196512 (90%)]\tLoss: 0.621753\n",
      "Train Epoch: 5 [179200/196512 (91%)]\tLoss: 0.597285\n",
      "Train Epoch: 5 [181248/196512 (92%)]\tLoss: 0.626929\n",
      "Train Epoch: 5 [183296/196512 (93%)]\tLoss: 0.590804\n",
      "Train Epoch: 5 [185344/196512 (94%)]\tLoss: 0.594831\n",
      "Train Epoch: 5 [187392/196512 (95%)]\tLoss: 0.631565\n",
      "Train Epoch: 5 [189440/196512 (96%)]\tLoss: 0.598838\n",
      "Train Epoch: 5 [191488/196512 (97%)]\tLoss: 0.615642\n",
      "Train Epoch: 5 [193536/196512 (98%)]\tLoss: 0.624802\n",
      "Train Epoch: 5 [177248/196512 (99%)]\tLoss: 0.633557\n",
      "\n",
      "Test set: Average loss: -164.4649, Accuracy: 31824/49128 (65%)\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "mini_cnn = MiniCNN(input_length).to(device)\n",
    "mini_optimizer = torch.optim.Adam(mini_cnn.parameters(), lr=0.0001)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(mini_cnn, device, train_dataloader, mini_optimizer, epoch)\n",
    "    test(mini_cnn, device, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
